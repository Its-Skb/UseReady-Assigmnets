{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3b2caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: python-docx in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: cohere in c:\\users\\asus\\anaconda3\\lib\\site-packages (5.15.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pytesseract) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-docx) (4.14.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-docx) (4.9.1)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (1.11.1)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (0.21.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (2.35.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (2.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (1.10.12)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (0.28.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cohere) (2.31.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (2024.7.4)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.5.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tokenizers<1,>=0.15->cohere) (0.33.2)\n",
      "Requirement already satisfied: types-urllib3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from types-requests<3.0.0,>=2.0.0->cohere) (1.26.25.14)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.21.2->cohere) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract python-docx cohere pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd82e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (37.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\asus\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce90b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42baccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_API_KEY = \"wAxVCab3xnABNI8EkReLEtEa7tnPUAiF7srUPoFC\"\n",
    "co = cohere.Client(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9d848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "tempfile.tempdir = \"C:/Users/ASUS/AppData/Local/Temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdecae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_png(filepath):\n",
    "    try:\n",
    "        image = Image.open(filepath).convert(\"L\")\n",
    "        return pytesseract.image_to_string(\n",
    "            image,\n",
    "            config=\"--psm 6\",\n",
    "            lang=\"eng\"\n",
    "        )\n",
    "    except pytesseract.TesseractNotFoundError as e:\n",
    "        print(f\"‚ùå Tesseract not found. Please ensure it is installed and in PATH. Skipping: {filepath}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading PNG file {filepath}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3beafbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_docx(filepath):\n",
    "    try:\n",
    "        doc = Document(filepath)\n",
    "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading DOCX file {filepath}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22cbe970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filepath):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading PDF file {filepath}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42c1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_full_filename(basename, folder):\n",
    "    for ext in [\".docx\", \".pdf\", \".png\"]:\n",
    "        full_path = os.path.join(folder, basename + ext)\n",
    "        if os.path.exists(full_path):\n",
    "            return basename + ext\n",
    "    return None  # Not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f5d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_text(text):\n",
    "    prompt = f\"\"\"\n",
    "You are an intelligent assistant trained to extract metadata from legal agreement documents.\n",
    "Extract the following fields:\n",
    "- agreement_value\n",
    "- start_date\n",
    "- end_date\n",
    "- renewal_notice_days\n",
    "- party_one\n",
    "- party_two\n",
    "\n",
    "### Example:\n",
    "Input:\n",
    "This Rental Agreement is made between John Doe and Jane Smith. The rent is $2500. It starts on 01.01.2022 and ends on 31.12.2022. Notice for renewal is 60 days prior.\n",
    "\n",
    "Output:\n",
    "{{\n",
    "  \"agreement_value\": \"2500\",\n",
    "  \"start_date\": \"01.01.2022\",\n",
    "  \"end_date\": \"31.12.2022\",\n",
    "  \"renewal_notice_days\": \"60\",\n",
    "  \"party_one\": \"John Doe\",\n",
    "  \"party_two\": \"Jane Smith\"\n",
    "}}\n",
    "\n",
    "### Now extract from the following document:\n",
    "{text}\n",
    "\n",
    "Output as JSON:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = co.generate(\n",
    "            model=\"command-r-plus\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=300,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.generations[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cohere API Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c7a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"data/train.csv\")\n",
    "# print(\"üìÅ Files in data/train/:\")\n",
    "# print(os.listdir(\"data/train\"))\n",
    "# print(train_df.head())\n",
    "\n",
    "# # Example: Process one file\n",
    "# filename = train_df.loc[0, \"File Name\"]\n",
    "# filepath = os.path.join(\"data/train\", filename)\n",
    "\n",
    "# if filename.endswith(\".png\"):\n",
    "#     raw_text = extract_text_from_png(filepath)\n",
    "# elif filename.endswith(\".docx\"):\n",
    "#     raw_text = extract_text_from_docx(filepath)\n",
    "#     print(\"\\n--- Raw Text Extracted ---\\n\")\n",
    "#     print(raw_text[:1000])  # First 1000 characters\n",
    "# else:\n",
    "#     raw_text = \"\"\n",
    "\n",
    "# # Extract metadata using Cohere\n",
    "# output = extract_metadata_from_text(raw_text)\n",
    "# print(\"\\nExtracted Metadata:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7273af9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                          | 3/10 [00:09<00:21,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå File not found for: 24158401-Rental-Agreement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                         | 5/10 [00:10<00:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error reading PNG file data/train\\36199312-Rental-Agreement.png: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Temp\\\\tess_lbsruptp.txt'\n",
      "‚ö†Ô∏è Too little text in: 36199312-Rental-Agreement.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:26<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved 8 predictions to output/train_predictions.csv\n",
      "üßæ Skipped due to missing train files: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "results = []\n",
    "skipped_missing = 0\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    basename = row[\"File Name\"]\n",
    "    filename = resolve_full_filename(basename, folder=\"data/train\")\n",
    "\n",
    "    if not filename:\n",
    "        print(f\"‚ùå File not found for: {basename}\")\n",
    "        skipped_missing += 1\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(\"data/train\", filename)\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "\n",
    "    if ext == \".png\":\n",
    "        raw_text = extract_text_from_png(filepath)\n",
    "    elif ext == \".docx\":\n",
    "        raw_text = extract_text_from_docx(filepath)\n",
    "    elif ext == \".pdf\":\n",
    "        raw_text = extract_text_from_pdf(filepath)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping unsupported file type: {filename}\")\n",
    "        continue\n",
    "\n",
    "    if len(raw_text.strip()) < 10:\n",
    "        print(f\"‚ö†Ô∏è Too little text in: {filename}\")\n",
    "        continue\n",
    "\n",
    "    metadata_json_str = extract_metadata_from_text(raw_text)\n",
    "\n",
    "    try:\n",
    "        metadata = json.loads(metadata_json_str) if metadata_json_str.strip().startswith(\"{\") else {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to parse model output for {filename}: {e}\")\n",
    "        metadata = {}\n",
    "\n",
    "    results.append({\n",
    "        \"File Name\": basename,\n",
    "        \"agreement_value\": metadata.get(\"agreement_value\"),\n",
    "        \"start_date\": metadata.get(\"start_date\"),\n",
    "        \"end_date\": metadata.get(\"end_date\"),\n",
    "        \"renewal_notice_days\": metadata.get(\"renewal_notice_days\"),\n",
    "        \"party_one\": metadata.get(\"party_one\"),\n",
    "        \"party_two\": metadata.get(\"party_two\"),\n",
    "    })\n",
    "\n",
    "# Save train output safely\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "try:\n",
    "    pd.DataFrame(results).to_csv(\"output/train_predictions.csv\", index=False)\n",
    "    print(f\"\\n‚úÖ Saved {len(results)} predictions to output/train_predictions.csv\")\n",
    "except PermissionError:\n",
    "    print(\"‚ùå Could not write to output/train_predictions.csv ‚Äî is it open in Excel?\")\n",
    "\n",
    "print(f\"üßæ Skipped due to missing train files: {skipped_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f28bf94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                               | 1/4 [00:00<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error reading PNG file data/test\\24158401-Rental-Agreement.png: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Temp\\\\tess_vv8bo0_i.txt'\n",
      "‚ö†Ô∏è Too little text in test file: 24158401-Rental-Agreement.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error reading PNG file data/test\\95980236-Rental-Agreement.png: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Temp\\\\tess_hjo65s37.txt'\n",
      "‚ö†Ô∏è Too little text in test file: 95980236-Rental-Agreement.png\n",
      "‚ùå Test file not found for: 156155545-Rental-Agreement-Kns-Home\n",
      "‚ùå Test file not found for: 228094620-Rental-Agreement\n",
      "\n",
      "‚úÖ Saved 0 predictions to output/test_predictions.csv\n",
      "üßæ Skipped due to missing test files: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "test_results = []\n",
    "skipped_test_missing = 0\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    basename = row[\"File Name\"]\n",
    "    filename = resolve_full_filename(basename, folder=\"data/test\")\n",
    "\n",
    "    if not filename:\n",
    "        print(f\"‚ùå Test file not found for: {basename}\")\n",
    "        skipped_test_missing += 1\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(\"data/test\", filename)\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "\n",
    "    if ext == \".png\":\n",
    "        raw_text = extract_text_from_png(filepath)\n",
    "    elif ext == \".docx\":\n",
    "        raw_text = extract_text_from_docx(filepath)\n",
    "    elif ext == \".pdf\":\n",
    "        raw_text = extract_text_from_pdf(filepath)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping unsupported test file type: {filename}\")\n",
    "        continue\n",
    "\n",
    "    if len(raw_text.strip()) < 10:\n",
    "        print(f\"‚ö†Ô∏è Too little text in test file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    metadata_json_str = extract_metadata_from_text(raw_text)\n",
    "\n",
    "    try:\n",
    "        metadata = json.loads(metadata_json_str) if metadata_json_str.strip().startswith(\"{\") else {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to parse model output for test file {filename}: {e}\")\n",
    "        metadata = {}\n",
    "\n",
    "    test_results.append({\n",
    "        \"File Name\": basename,\n",
    "        \"agreement_value\": metadata.get(\"agreement_value\"),\n",
    "        \"start_date\": metadata.get(\"start_date\"),\n",
    "        \"end_date\": metadata.get(\"end_date\"),\n",
    "        \"renewal_notice_days\": metadata.get(\"renewal_notice_days\"),\n",
    "        \"party_one\": metadata.get(\"party_one\"),\n",
    "        \"party_two\": metadata.get(\"party_two\"),\n",
    "    })\n",
    "\n",
    "# Save test output safely\n",
    "try:\n",
    "    pd.DataFrame(test_results).to_csv(\"output/test_predictions.csv\", index=False)\n",
    "    print(f\"\\n‚úÖ Saved {len(test_results)} predictions to output/test_predictions.csv\")\n",
    "except PermissionError:\n",
    "    print(\"‚ùå Could not write to output/test_predictions.csv ‚Äî is it open in Excel?\")\n",
    "\n",
    "print(f\"üßæ Skipped due to missing test files: {skipped_test_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7b1e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(pred_path=\"output/train_predictions.csv\", gt_path=\"data/train.csv\"):\n",
    "    # Load both files\n",
    "    gt_df = pd.read_csv(gt_path)\n",
    "    pred_df = pd.read_csv(pred_path)\n",
    "\n",
    "    # Rename ground truth columns to snake_case for comparison\n",
    "    gt_df = gt_df.rename(columns={\n",
    "        \"Aggrement Value\": \"agreement_value\",\n",
    "        \"Aggrement Start Date\": \"start_date\",\n",
    "        \"Aggrement End Date\": \"end_date\",\n",
    "        \"Renewal Notice (Days)\": \"renewal_notice_days\",\n",
    "        \"Party One\": \"party_one\",\n",
    "        \"Party Two\": \"party_two\"\n",
    "    })\n",
    "\n",
    "    # Merge on 'File Name'\n",
    "    merged = pd.merge(gt_df, pred_df, on=\"File Name\")\n",
    "\n",
    "    fields = [\n",
    "        \"agreement_value\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "        \"renewal_notice_days\",\n",
    "        \"party_one\",\n",
    "        \"party_two\"\n",
    "    ]\n",
    "\n",
    "    print(\"üìä Field-wise Recall:\\n\")\n",
    "    for field in fields:\n",
    "        true_values = merged[field + \"_x\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "        pred_values = merged[field + \"_y\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "\n",
    "        match = (true_values == pred_values) & (true_values != \"\")\n",
    "        total = (true_values != \"\").sum()\n",
    "        recall = match.sum() / total if total else 0\n",
    "\n",
    "        print(f\"{field:25s}: Recall = {recall:.2%} ({match.sum()} / {total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56696087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Field-wise Recall:\n",
      "\n",
      "agreement_value          : Recall = 37.50% (3 / 8)\n",
      "start_date               : Recall = 37.50% (3 / 8)\n",
      "end_date                 : Recall = 12.50% (1 / 8)\n",
      "renewal_notice_days      : Recall = 0.00% (0 / 7)\n",
      "party_one                : Recall = 0.00% (0 / 8)\n",
      "party_two                : Recall = 12.50% (1 / 8)\n"
     ]
    }
   ],
   "source": [
    "compute_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be894e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Columns in train.csv:\n",
      "['File Name', 'Aggrement Value', 'Aggrement Start Date', 'Aggrement End Date', 'Renewal Notice (Days)', 'Party One', 'Party Two']\n",
      "\n",
      "üìÑ Columns in output/train_predictions.csv:\n",
      "['File Name', 'agreement_value', 'start_date', 'end_date', 'renewal_notice_days', 'party_one', 'party_two']\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÑ Columns in train.csv:\")\n",
    "print(pd.read_csv(\"data/train.csv\").columns.tolist())\n",
    "\n",
    "print(\"\\nüìÑ Columns in output/train_predictions.csv:\")\n",
    "print(pd.read_csv(\"output/train_predictions.csv\").columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344bf26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
